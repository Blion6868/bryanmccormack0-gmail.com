{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMSH/c84pckscSH9vJ6gydL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Blion6868/bryanmccormack0-gmail.com/blob/master/Paradise_Lost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlfNoaAzZyuY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "b8da2e5b-75e1-46d6-c1a3-a0c9d5aae924"
      },
      "source": [
        "#basic imports\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hV17exzZ_Jn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\"Paradise Lost,\" by John Milton, courtesy of https://www.gutenberg.org/\n",
        "\n",
        "Milton_File = 'Milton.txt'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkPtpIlOdtZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read text file\n",
        "\n",
        "Milton_text = open(Milton_File, 'r').read()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqxRvHifZ_f4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "6d349a71-8dae-4b7a-e3ad-24580420e6f2"
      },
      "source": [
        "#to begin processing the text, we must create a vocabulary list\n",
        "\n",
        "vocabulary = sorted(set(Milton_text))\n",
        "print(vocabulary)\n",
        "len(vocabulary)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'â€™']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JxaHO6SZ_j7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#once the vocabulary list is created, we also need to index it\n",
        "\n",
        "vocabulary_index = {u:i for i, u in enumerate(vocabulary)}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqYm2BuXepNW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we need to create a numpy array of the vocabulary\n",
        "index_to_vocab = np.array(vocabulary)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E1FpzTBepSg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we have both an index # and each individual character; we now need to encode the character as a unique #\n",
        "encoded_Milton_text = np.array([vocabulary_index[c] for c in Milton_text])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aQAVqe2epcJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "046d64c6-8ac3-45e9-cffb-c7b06844f818"
      },
      "source": [
        "encoded_Milton_text[:20]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0, 49, 66, 63,  1, 45, 76, 73, 68, 63, 61, 78,  1, 36, 79, 78, 63,\n",
              "       72, 60, 63])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeB_jmE8epkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create training sequences\n",
        "\n",
        "vocabulary_dataset = tf.data.Dataset.from_tensor_slices(encoded_Milton_text)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3Nymmb0epZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to create the model that will mimic the inputted text, we need to create batches to feed into the model. The # of sequences is rather arbitrary and can be played with accordingly\n",
        "\n",
        "sequence_len = 120\n",
        "sequences = vocabulary_dataset.batch(sequence_len+1, drop_remainder=True)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxbJGDiFeWf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for the model to work correctly, we need to create a function that extracts the last character and skips the first character: \n",
        "# F Mans First Disobedience, and the Fruit--OF Mans First Disobedience, and the Frui\n",
        "\n",
        "def create_seq_targets(seq):\n",
        "    input_txt = seq[:-1]\n",
        "    target_txt = seq[1:]\n",
        "    return input_txt, target_txt"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuNWhu4Ghvm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = sequences.map(create_seq_targets)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n6EgaKQhvtY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "fb307142-c8b6-44a8-962a-5b0ea37e20d7"
      },
      "source": [
        "for input_txt, target_txt in  dataset.take(1):\n",
        "    print(input_txt.numpy())\n",
        "    print(''.join(index_to_vocab[input_txt.numpy()]))\n",
        "    print('\\n')\n",
        "    print(target_txt.numpy())\n",
        "    print(''.join(index_to_vocab[target_txt.numpy()]))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0 49 66 63  1 45 76 73 68 63 61 78  1 36 79 78 63 72 60 63 76 65  1 34\n",
            " 31 73 73 69  1 73 64  1 45 59 76 59 62 67 77 63  1 41 73 77 78 12  1 60\n",
            " 83  1 39 73 66 72  1 42 67 70 78 73 72  0  0 49 66 67 77  1 63 31 73 73\n",
            " 69  1 67 77  1 64 73 76  1 78 66 63  1 79 77 63  1 73 64  1 59 72 83 73\n",
            " 72 63  1 59 72 83 81 66 63 76 63  1 67 72  1 78 66 63  1 50 72 67 78 63]\n",
            "\n",
            "The Project Gutenberg EBook of Paradise Lost, by John Milton\n",
            "\n",
            "This eBook is for the use of anyone anywhere in the Unite\n",
            "\n",
            "\n",
            "[49 66 63  1 45 76 73 68 63 61 78  1 36 79 78 63 72 60 63 76 65  1 34 31\n",
            " 73 73 69  1 73 64  1 45 59 76 59 62 67 77 63  1 41 73 77 78 12  1 60 83\n",
            "  1 39 73 66 72  1 42 67 70 78 73 72  0  0 49 66 67 77  1 63 31 73 73 69\n",
            "  1 67 77  1 64 73 76  1 78 66 63  1 79 77 63  1 73 64  1 59 72 83 73 72\n",
            " 63  1 59 72 83 81 66 63 76 63  1 67 72  1 78 66 63  1 50 72 67 78 63 62]\n",
            "The Project Gutenberg EBook of Paradise Lost, by John Milton\n",
            "\n",
            "This eBook is for the use of anyone anywhere in the United\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoAJLfCthvys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batch size for training the model\n",
        "batch_size = 128\n",
        "\n",
        "# Buffer so model doesn't learn the pattern of the text\n",
        "buffer_size = 10000\n",
        "\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFDjlBKahvrA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "187d0702-caa8-46e3-eeca-f03317f85475"
      },
      "source": [
        "dataset"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((128, 120), (128, 120)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwuForivhvjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in characters\n",
        "vocab_size = len(vocabulary)\n",
        "\n",
        "# The embedding dimension\n",
        "embed_dim = 64\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_neurons = 1050"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpTFFBEWjo2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM,Dense,Embedding,Dropout,GRU\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3TX_c3DjozO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to help build the model, we need to create own own loss function. logits refers to creating a map of probabilties values\n",
        "\n",
        "def sparse_cat_loss(y_true,y_pred):\n",
        "  return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z6mN_KVjoxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#building th TF model\n",
        "\n",
        "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embed_dim,batch_input_shape=[batch_size, None]))\n",
        "    model.add(GRU(rnn_neurons,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n",
        "    # Final Dense Layer to Predict\n",
        "    model.add(Dense(vocab_size))\n",
        "    model.compile(optimizer='adam', loss=sparse_cat_loss) \n",
        "    return model"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef7xoQ2Qjosd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = create_model(\n",
        "  vocab_size = vocab_size,\n",
        "  embed_dim=embed_dim,\n",
        "  rnn_neurons=rnn_neurons,\n",
        "  batch_size=batch_size)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7oMzdVojooW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "882047de-30f3-4cdf-c8e3-b6e8fa1e6c64"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (128, None, 64)           5504      \n",
            "_________________________________________________________________\n",
            "gru_7 (GRU)                  (128, None, 1050)         3515400   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (128, None, 86)           90386     \n",
            "=================================================================\n",
            "Total params: 3,611,290\n",
            "Trainable params: 3,611,290\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU8Vm1lMkqVG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c1392c98-6375-4413-c55d-5d05addd64a1"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "\n",
        "  # Predict off some random batch\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "\n",
        "  # Display the dimensions of the predictions\n",
        "  print(example_batch_predictions.shape)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128, 120, 86)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggMqCuQ-kqX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QOH4OoIkqdf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reformat to not be a list of lists\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R86Qd6ASkqjO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e22c38a5-b160-4de5-b149-ac8096be8c2e"
      },
      "source": [
        "epochs = 30\n",
        "\n",
        "model.fit(dataset,epochs=epochs)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "30/30 [==============================] - 252s 8s/step - loss: 3.8422\n",
            "Epoch 2/30\n",
            "30/30 [==============================] - 248s 8s/step - loss: 3.1440\n",
            "Epoch 3/30\n",
            "30/30 [==============================] - 253s 8s/step - loss: 2.8905\n",
            "Epoch 4/30\n",
            "30/30 [==============================] - 249s 8s/step - loss: 2.5886\n",
            "Epoch 5/30\n",
            "30/30 [==============================] - 250s 8s/step - loss: 2.4491\n",
            "Epoch 6/30\n",
            "30/30 [==============================] - 249s 8s/step - loss: 2.3729\n",
            "Epoch 7/30\n",
            "30/30 [==============================] - 249s 8s/step - loss: 2.3117\n",
            "Epoch 8/30\n",
            "30/30 [==============================] - 246s 8s/step - loss: 2.2571\n",
            "Epoch 9/30\n",
            "30/30 [==============================] - 248s 8s/step - loss: 2.2001\n",
            "Epoch 10/30\n",
            "30/30 [==============================] - 245s 8s/step - loss: 2.1419\n",
            "Epoch 11/30\n",
            "30/30 [==============================] - 249s 8s/step - loss: 2.0819\n",
            "Epoch 12/30\n",
            "30/30 [==============================] - 246s 8s/step - loss: 2.0224\n",
            "Epoch 13/30\n",
            "30/30 [==============================] - 248s 8s/step - loss: 1.9641\n",
            "Epoch 14/30\n",
            "30/30 [==============================] - 255s 9s/step - loss: 1.9110\n",
            "Epoch 15/30\n",
            "30/30 [==============================] - 252s 8s/step - loss: 1.8619\n",
            "Epoch 16/30\n",
            "30/30 [==============================] - 259s 9s/step - loss: 1.8134\n",
            "Epoch 17/30\n",
            "30/30 [==============================] - 249s 8s/step - loss: 1.7704\n",
            "Epoch 18/30\n",
            "30/30 [==============================] - 247s 8s/step - loss: 1.7275\n",
            "Epoch 19/30\n",
            "30/30 [==============================] - 251s 8s/step - loss: 1.6886\n",
            "Epoch 20/30\n",
            "30/30 [==============================] - 250s 8s/step - loss: 1.6508\n",
            "Epoch 21/30\n",
            "30/30 [==============================] - 256s 9s/step - loss: 1.6169\n",
            "Epoch 22/30\n",
            "30/30 [==============================] - 252s 8s/step - loss: 1.5844\n",
            "Epoch 23/30\n",
            "30/30 [==============================] - 254s 8s/step - loss: 1.5541\n",
            "Epoch 24/30\n",
            "30/30 [==============================] - 250s 8s/step - loss: 1.5231\n",
            "Epoch 25/30\n",
            "30/30 [==============================] - 253s 8s/step - loss: 1.4930\n",
            "Epoch 26/30\n",
            "30/30 [==============================] - 252s 8s/step - loss: 1.4654\n",
            "Epoch 27/30\n",
            "30/30 [==============================] - 254s 8s/step - loss: 1.4375\n",
            "Epoch 28/30\n",
            "30/30 [==============================] - 249s 8s/step - loss: 1.4113\n",
            "Epoch 29/30\n",
            "30/30 [==============================] - 250s 8s/step - loss: 1.3826\n",
            "Epoch 30/30\n",
            "30/30 [==============================] - 249s 8s/step - loss: 1.3569\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f550bd634e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eEn_nA3kqhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('Milton_gen.h5') "
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6gxbNirkqbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpboX6lJkqT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)\n",
        "\n",
        "model.load_weights('Milton_gen.h5')\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDSbI2gUkqPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_seed,gen_size=100,temp=1.0):\n",
        "  # Number of characters to generate\n",
        "  num_generate = gen_size\n",
        "\n",
        "  # Vecotrizing starting seed text\n",
        "  input_eval = [vocabulary_index[s] for s in start_seed]\n",
        "\n",
        "  # Expand to match batch format shape\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty list to hold resulting generated text\n",
        "  text_generated = []\n",
        "\n",
        "  temperature = temp\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "\n",
        "  for i in range(num_generate):\n",
        "\n",
        "      # Generate Predictions\n",
        "      predictions = model(input_eval)\n",
        "\n",
        "      # Remove the batch shape dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # Use a cateogircal disitribution to select the next character\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # Pass the predicted charracter for the next input\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      # Transform back to character letter\n",
        "      text_generated.append(vocabulary_index[predicted_id])\n",
        "\n",
        "  return (start_seed + ''.join(text_generated))"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqMiHv_flUsp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "4e1b2a5d-b0d6-40bf-e8b3-886040f1925f"
      },
      "source": [
        "print(generate_text(model,\"Satan\",gen_size=1000))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-25f1950b1147>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Satan\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgen_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-69-f28f1c0bcb45>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model, start_seed, gen_size, temp)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;31m# Transform back to character letter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m       \u001b[0mtext_generated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicted_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart_seed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_generated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 74"
          ]
        }
      ]
    }
  ]
}